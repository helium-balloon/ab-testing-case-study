<!DOCTYPE html>
<html lang="en">
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>A/B Testing Case Study</title>
    <link rel="stylesheet" href="styles.css" />
    <link
      href="https://fonts.googleapis.com/css?family=Rubik"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css?family=Gothic A1"
      rel="stylesheet"
    />
  </head>

  <body>
    <header>
      <h1>A/B Testing Case Study</h1>
      <p>
        In this project, my main goal was to determine whether one user
        interface design is more effective than another in improving the
        usability for the user. To address this question, I explored A/B
        testing, where I compared two version of the user interface and compared
        which performed better through the testing of my peers. By measuring key
        metrics, I am able to statistically figure out which interface is more
        successful for the user's needs. Read on to learn about my process and
        my results!
      </p>
    </header>
    <main>
      <div>
        <h2>A/B Testing Studio</h2>
        <p>
          To begin, my peers and I first used the Version A website to schedule
          an appointment without seeing the interface ahead of time. Then, after
          using the interface, I made small changes to the interface, including
          changing the color of the buttons to make them more readable and
          clear, and changing the boldness of the location of the appointment to
          reduce confusion. After making these changes, I tested my website for
          speed and accuracy by having others use it to complete the same task
          of scheduling an appointment.
        </p>
        <div class="half">
          <div class="center">
            <h3>Version A (Original)</h3>
            <img src="./assets/versionA.png" alt="original design" />
          </div>
          <div class="center">
            <h3>Version B (Edited)</h3>
            <img src="./assets/versionB.png" alt="changed design" />
          </div>
        </div>
      </div>

      <div>
        <h2>Hypotheses</h2>
        <p>
          Using the data collected during the A/B Testing studio, I made
          hypotheses based on what I believe I will find after testing the data.
        </p>
        <div>
          <h3>Metrics</h3>
          <ul class="list">
            <li>
              <b>Misclick Rate</b> - the frequency with which users click
              something else on the page before finding the correct button for
              the task
            </li>
            <li>
              <b>Time on Page</b> - time spent on the webpage for each user
              group
            </li>
            <li>
              <b>Number of Clicks</b> - how many times users clicked on the page
            </li>
          </ul>
          <p>
            My metric of choice is the number of clicks users used to complete
            the task. I chose this because I noticed that there was a large
            variety of clicks used to complete the task, especially in the A
            task, and I wanted to see what the data looked like comparing the
            two in terms of clicks. In addition, I think that this is a useful
            metric because if it takes less clicks, then it can be easier for
            the user to complete.
          </p>
        </div>

        <div>
          <h3>Null and Alternative Hypotheses</h3>
          <ul class="list">
            <li><b>Misclick Rate</b></li>
            <li style="list-style-type: none">
              <ul class="circle">
                <li>
                  <i>Null Hypothesis -</i> The misclick rate is the same in
                  sample A and sample B.
                </li>
                <li>
                  <i>Alternative Hypothesis -</i> The misclick rate from sample
                  A is different than the misclick rate from sample B.
                </li>
                <li>
                  <i>Justification - </i>I made this alternative hypothesis
                  because in version A, no one knew how the website worked and
                  it was a confusing interface so more people were more likely
                  to misclick. However, in version B, the interface was more
                  clear and understandable, so it is likely that there was a
                  smaller misclick rate, so the misclick rate will likely
                  differ.
                </li>
              </ul>
            </li>
            <li><b>Time on Page</b></li>
            <li style="list-style-type: none">
              <ul class="circle">
                <li>
                  <i>Null Hypothesis -</i> The time spent on page is the same in
                  sample A and sample B.
                </li>
                <li>
                  <i>Alternative Hypothesis -</i> The time spent on page is
                  greater in sample A than in sample B.
                </li>
                <li>
                  <i>Justification -</i> I made this alternative hypothesis
                  because in version A, people are more likely to make mistakes
                  because the interface is new and confusing, leading them to
                  spend more time on the page especially if they misclick and
                  make mistakes. On the other hand, if version B is easier to
                  use and understand, users will be able to complete the task in
                  a shorter amount of time because the steps are more clear.
                </li>
              </ul>
            </li>
            <li><b>Number of Clicks</b></li>
            <li style="list-style-type: none">
              <ul class="circle">
                <li>
                  Null Hypothesis: The number of clicks used to complete the
                  task is the same in sample A and sample B.
                </li>
                <li>
                  Alternative Hypothesis: The number of clicks used to complete
                  the task is greater in sample A than sample B.
                </li>
                <li>
                  <i>Justification -</i> I made this alternative hypothesis
                  because in version A, people are more likely to make mistakes
                  and end up clicking multiple things, which will increase the
                  number of clicks. On the other hand, in version B, people will
                  be more likely to find what they are looking for and click a
                  smaller amount of times to get there because the interface is
                  more intuitive through making the buttons and hospital
                  locations more readable.
                </li>
              </ul>
            </li>
          </ul>
        </div>

        <div>
          <h3>Predictions</h3>
          <ul class="list">
            <li><b>Misclick Rate</b></li>
            <li style="list-style-type: none">
              <ul class="circle">
                <li>
                  I believe that I will end up rejecting the null hypothesis
                  based on the data. In the data from Version A, about one half
                  of people misclicked at least once during their attempt.
                  However, during Version B testing, no users misclicked.
                  Through testing this data, I expect that the p-value will
                  indicate that the difference between these two findings is
                  significant. Because of this, I expect to reject the null
                  hypothesis that the misclick rate for Version A and Version B
                  are the same.
                </li>
              </ul>
            </li>
            <li><b>Time on Page</b></li>
            <li style="list-style-type: none">
              <ul class="circle">
                <li>
                  I believe that I will end up rejecting the null hypothesis
                  after looking at the data for each version. From version A
                  testing, the majority of data points are over 20,000
                  milliseconds. However, in Version B testing, the users rarely
                  exceeded 10,000 milliseconds on the page. Due to this large
                  difference I witnessed in the data, I expect that through the
                  tests, the p-value will indicate that the amount of decrease
                  in time on page from Version A to Version B is significant .
                  Thus, I expect to reject the null hypothesis which states that
                  the time on page in Version A and Version B is the same.
                </li>
              </ul>
            </li>
            <li><b>Number of Clicks</b></li>
            <li style="list-style-type: none">
              <ul class="circle">
                <li>
                  I believe that I will end up rejecting the null hypothesis
                  through viewing the data from each version. I noticed that in
                  the version A data, there was a large difference in the number
                  of clicks different people took to complete the task, ranging
                  from 2 to 26. On the other hand, in the Version B data, there
                  was little variability in the number of clicks, as people only
                  used 2-4 clicks. Because of this large difference in the
                  number of clicks used in the different versions, I believe
                  that the p-value will indicate that the amount of decrease in
                  the number of clicks from Version A to Version B is
                  significant. Through this, I expect to reject the null
                  hypothesis which states that the number of clicks in Version A
                  and Version B is the same.
                </li>
              </ul>
            </li>
          </ul>
        </div>
      </div>

      <div>
        <h2>Statistical Tests</h2>
        <p>
          After creating my hypotheses, I ran statistical tests on the metrics
          explained above to learn the impact of my A/B testing.
        </p>
        <div>
          <h3>Misclick Rate</h3>
          <div class="half">
            <p>
              For the misclick rate, I chose to run a chi squared test because
              misclick can be represented as a boolean value which can be split
              into two categories: did misclick and did not misclick. In sample
              A, 12 users misclicked and 12 users did not misclick. In sample B,
              0 users misclicked and 22 users did not misclick. <br /><br />

              Because the p-value is less than 0.05, the difference between versions A and B
              with respect to misclick rate is significant. We can come to the
              same conclusion through using the chi square statistic as well,
              which is 14.88. Using the degree of freedom, we can find the
              critical value of chi^2 to be 3.841, which is less than the chi^2
              value, so the magnitude of difference between the two groups is
              significant. <br /><br />

              Because the p-value is significant and the chi-square statistic is
              significant, we find statistically significant evidence that the
              alternative hypothesis is true.
            </p>
            <div class="center">
              <table>
                <tr>
                  <th>Outputs</th>
                </tr>
                <tr>
                  <td>df</td>
                  <td>1</td>
                </tr>
                <tr>
                  <td>chi^2</td>
                  <td>14.88</td>
                </tr>
                <tr>
                  <td>p-value</td>
                  <td>0.00011</td>
                </tr>
              </table>
            </div>
          </div>
        </div>
        <div>
          <h3>Time on Page</h3>
          <div class="half">
            <p>
              I chose to run a one-tailed t-test for the time on page because
              this piece of information is a number, not a category, and I
              believe that the most important information can be gleaned from
              understanding if the version B time on page is smaller than
              version A. Doing a two-tailed t-test would tell me only if they
              are significantly different, but in this case I want to see if
              version B is an improvement. <br /><br />

              Using this test, the outputs are noted in the table. These results
              show that the degrees of freedom for this metric is approximately
              24. This means that there are 24 pieces of information to estimate
              the variability in the data which impacts the critical values of
              the t-distribution, and the higher degrees of freedom, the closer
              the t-distribution resembles the normal distribution. Next, the
              t-score is 9.578, and the p-value when A is less than B is
              0.99999. Because my alternative hypothesis states that A>B, and
              these outputs are for when A is less than B, I will do 1-p to get
              my p-value of approximately 0.00000. <br /><br />

              Because the p-value for this metric is less than or equal to the
              chosen significan level, 0.05, the amount that A is larger than B
              is significant for the time spend on the page. Furthermore, using
              the t-value and the degrees of freedom, I can confirm this
              p-value. <br /><br />

              Because the p-value is significant, I have found statistically
              significant evidence that the alternative hypothesis is true.
            </p>
            <div class="center">
              <table>
                <tr>
                  <th>Outputs</th>
                </tr>
                <tr>
                  <td>df</td>
                  <td>24</td>
                </tr>
                <tr>
                  <td>t-value</td>
                  <td>9.578</td>
                </tr>
                <tr>
                  <td>p-value</td>
                  <td>0.0000000004</td>
                </tr>
              </table>
            </div>
          </div>
        </div>
        <div>
          <h3>Number of Clicks</h3>
          <div class="half">
            <p>
              I chose to run a one-tailed test for the number of clicks on a
              page because this statistic is a number, not a category, and I
              believe that important information can be taken away from this
              metric through understanding if the number of clicks in version B
              is smaller than the number of clicks in version A. The degrees of
              freedom for this metric is approximately 23, the T-score is 4.12,
              and the p-value is 0.9997. Similar to time on page, this data
              represents when A is less than B. Similar to time on page, this
              data represents A is smaller than B, so I inverted the data to
              analyze for my alternative hypothesis which is A is greater than
              B. In this case, the p-value equals 0.00021. <br /><br />

              Because the p-value is less than or equal to 0.05, we find that
              the amount that A is larger than B is significant for the number
              of clicks used to complete the task. Furthermore, using the
              t-value and the degrees of freedom, I can confirm this p-value.
              <br />
              <br />

              Because the p-value is significant we find statistically
              significant evidence that the alternative hypothesis is true.
            </p>
            <div class="center">
              <table>
                <tr>
                  <th>Outputs</th>
                </tr>
                <tr>
                  <td>df</td>
                  <td>23</td>
                </tr>
                <tr>
                  <td>t-value</td>
                  <td>4.11588</td>
                </tr>
                <tr>
                  <td>p-value</td>
                  <td>0.00021</td>
                </tr>
              </table>
            </div>
          </div>
        </div>
      </div>

      <div>
        <h2>Summary Statistics</h2>
      </div>

      <div>
        <h2>Conclusions</h2>
        <p>
          Through these three metrics, it is likely that version B is better
          than version A. We see that on average people spend less time
          completing the task, less misclicks, and less clicks overall using
          version B. I believe that this difference is due to the changes in the
          interface in version B, where the two types of options that can be
          taken “See Appointment” and “Schedule Appointment” are visually much
          different. In addition, the location of the appointment is made
          clearer. Doing a two-tailed t-test would tell me if they are
          significantly different, but in this case where I want to see if
          version B is an improvement, it is important to me to understand if
          version B is smaller than version A. The results from the test are
          indicated in the table to the right.
        </p>
      </div>
    </main>
  </body>
</html>
